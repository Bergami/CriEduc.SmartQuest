# üîÑ Remover Sistema de Cache e Implementar Verifica√ß√£o de Duplicatas no MongoDB

## üìã Descri√ß√£o

Substituir o sistema de cache baseado em arquivos por verifica√ß√£o de duplicatas diretamente no MongoDB. Quando um documento com mesmo `email + filename` (e opcionalmente `file_size`) j√° existir no banco, retornar os dados j√° processados sem reprocessar.

## üéØ Objetivos

1. **Remover completamente** o sistema de cache de arquivos (`app/core/cache/`)
2. **Implementar verifica√ß√£o de duplicatas** no MongoDB antes do processamento
3. **Retornar dados existentes** quando documento j√° foi processado
4. **Permitir reprocessamento** quando status anterior for `FAILED`
5. **Criar √≠ndices MongoDB** para performance otimizada

## üîç An√°lise de Impacto

### **Componentes Afetados**

| Componente                     | Tipo de Impacto            | Complexidade | Esfor√ßo Estimado    |
| ------------------------------ | -------------------------- | ------------ | ------------------- |
| `app/core/cache/` (4 arquivos) | **REMO√á√ÉO COMPLETA**       | Alta         | 4h                  |
| `DocumentExtractionService`    | **REFATORA√á√ÉO CR√çTICA**    | Alta         | 6h                  |
| `MongoDBPersistenceService`    | **NOVA FUNCIONALIDADE**    | M√©dia        | 4h                  |
| `analyze.py` controller        | **L√ìGICA DE VERIFICA√á√ÉO**  | M√©dia        | 3h                  |
| `AnalyzeDocumentRecord` model  | **NOVO CAMPO (file_size)** | Baixa        | 1h                  |
| Migrations MongoDB             | **NOVO √çNDICE**            | Baixa        | 2h                  |
| Testes unit√°rios               | **NOVOS CEN√ÅRIOS**         | Alta         | 8h                  |
| Testes de integra√ß√£o           | **VALIDA√á√ÉO E2E**          | M√©dia        | 4h                  |
| Documenta√ß√£o                   | **ATUALIZA√á√ÉO**            | Baixa        | 2h                  |
| **TOTAL**                      | -                          | -            | **34h (~4-5 dias)** |

### **Arquivos a Remover**

```
app/core/cache/
‚îú‚îÄ‚îÄ __init__.py                 ‚ùå DELETAR
‚îú‚îÄ‚îÄ cache_decorator.py          ‚ùå DELETAR
‚îú‚îÄ‚îÄ cache_key_builder.py        ‚ùå DELETAR
‚îú‚îÄ‚îÄ cache_manager.py            ‚ùå DELETAR
‚îî‚îÄ‚îÄ cache_storage.py            ‚ùå DELETAR

Raiz do projeto:
‚îú‚îÄ‚îÄ cache_manager_cli.py        ‚ùå DELETAR
‚îú‚îÄ‚îÄ test_cache_system.py        ‚ùå DELETAR
‚îî‚îÄ‚îÄ cache/ (diret√≥rio)          ‚ùå DELETAR
```

### **Arquivos a Modificar**

```
app/services/extraction/document_extraction_service.py   üîß REFATORAR
app/services/persistence/mongodb_persistence_service.py  ‚ûï ADICIONAR
app/services/persistence/i_simple_persistence_service.py ‚ûï ADICIONAR
app/api/controllers/analyze.py                           üîß MODIFICAR
app/models/persistence/analyze_document_record.py        ‚ûï ADICIONAR
scripts/migrations/                                      ‚ûï NOVA MIGRATION
```

## üìä Decis√µes de Design

### **1. Crit√©rio de Duplicata**

**An√°lise de Impacto: `file_size` vs `hash`**

| Crit√©rio                       | Precis√£o           | Performance | Complexidade | Recomenda√ß√£o     |
| ------------------------------ | ------------------ | ----------- | ------------ | ---------------- |
| `email + filename`             | ‚ö†Ô∏è Baixa (80%)     | ‚úÖ R√°pida   | ‚úÖ Simples   | ‚ùå Insuficiente  |
| `email + filename + file_size` | ‚úÖ Alta (95%)      | ‚úÖ R√°pida   | ‚úÖ Simples   | ‚úÖ **ESCOLHIDA** |
| `email + filename + hash`      | ‚úÖ Perfeita (100%) | ‚ö†Ô∏è Lenta    | ‚ö†Ô∏è Complexa  | ‚ùå Overkill      |

**Decis√£o:** `email + filename + file_size`

**Justificativa:**

- ‚úÖ Detecta 95% das mudan√ßas de conte√∫do (tamanho diferente = arquivo diferente)
- ‚úÖ Performance excelente (integer comparison)
- ‚úÖ √çndice composto MongoDB eficiente
- ‚úÖ Implementa√ß√£o simples sem hash computation
- ‚ö†Ô∏è Edge case raro: Arquivo editado mantendo mesmo tamanho (aceit√°vel)

### **2. Comportamento de Retorno**

**Status HTTP:** `200 OK` (n√£o √© erro, √© otimiza√ß√£o)

**Response Body:**

```json
{
  "status": "already_processed",
  "message": "Documento j√° foi processado anteriormente em 2025-11-20T10:30:00Z",
  "document_id": "423a02fd-a0e0-4392-b66a-a43250e51ac3",
  "processed_at": "2025-11-20T10:30:00.319Z",
  "from_cache": false,
  "from_database": true,
  "analysis_results": {
    "document_id": "de8648f0-b36e-4513-9ca4-b11ad6cc2f25",
    "email": "professor@escola.edu.br",
    "filename": "Recuperacao.pdf",
    "questions": [...],
    "context_blocks": [...]
  }
}
```

### **3. Casos de Borda**

| Cen√°rio               | Comportamento                | Justificativa                |
| --------------------- | ---------------------------- | ---------------------------- |
| `status == COMPLETED` | Retornar dados existentes    | Otimiza√ß√£o - n√£o reprocessar |
| `status == FAILED`    | **Permitir reprocessamento** | Dar chance de sucesso        |
| `status == PENDING`   | Retornar erro 409 (Conflict) | Processamento em andamento   |
| Documento n√£o existe  | Processar normalmente        | Primeiro processamento       |

## üèóÔ∏è Arquitetura Proposta

### **Fluxo Atual (COM Cache)**

```mermaid
graph TB
    A[Upload PDF] --> B{Cache Check<br/>arquivo}
    B -->|HIT| C[Retornar cache]
    B -->|MISS| D[Extrair Azure]
    D --> E[Salvar Cache]
    E --> F[Processar]
    F --> G[Salvar MongoDB]
    G --> H[Response]
    C --> H
```

### **Fluxo Proposto (SEM Cache)**

```mermaid
graph TB
    A[Upload PDF] --> B{MongoDB Check<br/>email+filename+size}
    B -->|EXISTS + COMPLETED| C[Retornar dados DB]
    B -->|EXISTS + FAILED| D[Reprocessar]
    B -->|NOT EXISTS| D
    D --> E[Extrair Azure]
    E --> F[Processar]
    F --> G[Salvar MongoDB]
    G --> H[Response]
    C --> H
```

## üîß Implementa√ß√£o Detalhada

### **Fase 1: Prepara√ß√£o (2h)**

#### 1.1 Criar nova migration MongoDB

**Arquivo:** `scripts/migrations/2025_11_22_001000_add_file_size_and_duplicate_index.py`

```python
"""
Migration: Adicionar campo file_size e √≠ndice de duplicatas

Data: 2025-11-22
Autor: Sistema
"""

async def upgrade(db):
    """Aplicar migration."""
    collection = db["analyze_documents"]

    # 1. Adicionar campo file_size (opcional, para docs existentes)
    await collection.update_many(
        {"file_size": {"$exists": False}},
        {"$set": {"file_size": 0}}  # Docs antigos recebem 0 (desconhecido)
    )

    # 2. Criar √≠ndice composto para verifica√ß√£o de duplicatas
    await collection.create_index(
        [
            ("user_email", 1),
            ("file_name", 1),
            ("file_size", 1)
        ],
        name="idx_duplicate_check",
        background=True
    )

    # 3. Criar √≠ndice para queries por status
    await collection.create_index(
        [("status", 1)],
        name="idx_status",
        background=True
    )

    print("‚úÖ Migration 2025_11_22_001000 applied successfully")

async def downgrade(db):
    """Reverter migration."""
    collection = db["analyze_documents"]

    await collection.drop_index("idx_duplicate_check")
    await collection.drop_index("idx_status")

    print("‚úÖ Migration 2025_11_22_001000 reverted successfully")
```

#### 1.2 Atualizar model `AnalyzeDocumentRecord`

**Arquivo:** `app/models/persistence/analyze_document_record.py`

```python
# Adicionar novo campo
file_size: int = Field(default=0, description="Tamanho do arquivo em bytes")

# Atualizar m√©todo create_from_request
@classmethod
def create_from_request(
    cls,
    user_email: str,
    file_name: str,
    file_size: int,  # üÜï NOVO PAR√ÇMETRO
    response: Dict[str, Any],
    status: DocumentStatus = DocumentStatus.PENDING
):
    return cls(
        user_email=user_email,
        file_name=file_name,
        file_size=file_size,  # üÜï NOVO CAMPO
        response=response,
        status=status
    )
```

### **Fase 2: Servi√ßo de Persist√™ncia (4h)**

#### 2.1 Atualizar interface

**Arquivo:** `app/services/persistence/i_simple_persistence_service.py`

```python
async def check_duplicate_document(
    self,
    email: str,
    filename: str,
    file_size: int
) -> Optional[AnalyzeDocumentRecord]:
    """
    Verifica se documento j√° foi processado.

    Args:
        email: Email do usu√°rio
        filename: Nome do arquivo
        file_size: Tamanho do arquivo em bytes

    Returns:
        AnalyzeDocumentRecord se encontrado, None caso contr√°rio
    """
    pass
```

#### 2.2 Implementar verifica√ß√£o no MongoDB

**Arquivo:** `app/services/persistence/mongodb_persistence_service.py`

```python
async def check_duplicate_document(
    self,
    email: str,
    filename: str,
    file_size: int
) -> Optional[AnalyzeDocumentRecord]:
    """
    Verifica duplicata usando √≠ndice otimizado.

    Busca documento com:
    - Mesmo email
    - Mesmo filename
    - Mesmo file_size
    - Status COMPLETED (docs FAILED s√£o ignorados para permitir retry)

    Performance: O(1) devido ao √≠ndice composto
    """
    try:
        database = await self._connection_service.get_database()
        collection = database["analyze_documents"]

        # Query otimizada com √≠ndice idx_duplicate_check
        query = {
            "user_email": email,
            "file_name": filename,
            "file_size": file_size,
            "status": DocumentStatus.COMPLETED.value  # Apenas docs bem-sucedidos
        }

        # Buscar documento (usa √≠ndice)
        doc = await collection.find_one(query)

        if doc:
            self._logger.info({
                "event": "duplicate_document_found",
                "email": email,
                "filename": filename,
                "file_size": file_size,
                "document_id": str(doc["_id"]),
                "processed_at": doc.get("created_at")
            })

            # Converter para model Pydantic
            return AnalyzeDocumentRecord(**doc)

        return None

    except Exception as e:
        self._logger.error({
            "event": "duplicate_check_error",
            "error": str(e)
        })
        # Em caso de erro, retornar None para permitir processamento
        # (fail-safe: melhor reprocessar que bloquear)
        return None
```

### **Fase 3: Refatorar DocumentExtractionService (6h)**

**Arquivo:** `app/services/extraction/document_extraction_service.py`

**ANTES (com cache):**

```python
from app.core.cache import DocumentCacheManager

class DocumentExtractionService:
    @staticmethod
    async def get_extraction_data(file: UploadFile, email: str) -> Dict[str, Any]:
        cache_manager = DocumentCacheManager()

        # 1. Verificar cache
        await file.seek(0)
        cached_result = await cache_manager.get_cached_document(email, file)
        if cached_result:
            logger.info(f"üéØ Cache HIT")
            await file.seek(0)
            return cached_result.get("extracted_data")

        # 2. Extrair do Azure
        await file.seek(0)
        extractor = DocumentExtractionFactory.get_provider()
        extracted_data = await extractor.extract_document_data(file)

        # 3. Salvar no cache
        if extracted_data:
            await file.seek(0)
            await cache_manager.cache_document_result(email, file, extracted_data)

        await file.seek(0)
        return extracted_data
```

**DEPOIS (sem cache):**

```python
# ‚ùå REMOVER: from app.core.cache import DocumentCacheManager

class DocumentExtractionService:
    @staticmethod
    async def get_extraction_data(file: UploadFile, email: str) -> Dict[str, Any]:
        """
        Extrai dados do documento usando Azure Document Intelligence.

        NOTA: Verifica√ß√£o de duplicatas agora √© feita no controller
        antes de chamar este m√©todo.
        """
        # Resetar ponteiro do arquivo
        await file.seek(0)

        # Extrair do provedor (Azure)
        extractor = DocumentExtractionFactory.get_provider()
        extracted_data = await extractor.extract_document_data(file)

        # Resetar ponteiro para pr√≥ximo consumidor
        await file.seek(0)

        return extracted_data
```

### **Fase 4: Atualizar Controller (3h)**

**Arquivo:** `app/api/controllers/analyze.py`

```python
@router.post("/analyze_document")
@handle_exceptions("document_analysis")
async def analyze_document(
    request: Request,
    email: str = Query(...),
    file: UploadFile = File(...)
):
    """
    Analisa documento PDF com verifica√ß√£o de duplicatas.

    Fluxo:
    1. Valida√ß√£o
    2. üÜï Verificar duplicata no MongoDB
    3. Se duplicado e COMPLETED: retornar dados existentes
    4. Se duplicado e FAILED: reprocessar
    5. Se novo: processar normalmente
    """
    # 1. Valida√ß√£o
    await AnalyzeValidator.validate_all(email, file)

    # üÜï 2. Verificar duplicata no MongoDB
    persistence_service = container.resolve(ISimplePersistenceService)

    # Obter tamanho do arquivo
    await file.seek(0, 2)  # Ir para o final
    file_size = file.file.tell()
    await file.seek(0)  # Voltar ao in√≠cio

    existing_doc = await persistence_service.check_duplicate_document(
        email=email,
        filename=file.filename,
        file_size=file_size
    )

    # üÜï 3. Se documento j√° existe e foi processado com sucesso
    if existing_doc and existing_doc.status == DocumentStatus.COMPLETED:
        logger.info({
            "event": "duplicate_document_returned",
            "email": email,
            "filename": file.filename,
            "file_size": file_size,
            "document_id": str(existing_doc.id),
            "processed_at": existing_doc.created_at
        })

        return {
            "status": "already_processed",
            "message": f"Documento j√° foi processado anteriormente em {existing_doc.created_at.isoformat()}",
            "document_id": str(existing_doc.id),
            "processed_at": existing_doc.created_at,
            "from_cache": False,
            "from_database": True,
            "analysis_results": existing_doc.response
        }

    # üÜï 4. Log se for reprocessamento de documento falhado
    if existing_doc and existing_doc.status == DocumentStatus.FAILED:
        logger.info({
            "event": "reprocessing_failed_document",
            "email": email,
            "filename": file.filename,
            "previous_failure": str(existing_doc.id)
        })

    # 5. Processar documento (novo ou retry de falha)
    # ... resto do c√≥digo permanece igual
```

### **Fase 5: Remover Sistema de Cache (4h)**

#### 5.1 Arquivos a deletar

```bash
# M√≥dulo de cache
rm -rf app/core/cache/

# CLI e testes
rm cache_manager_cli.py
rm test_cache_system.py

# Diret√≥rio de cache
rm -rf cache/
```

#### 5.2 Limpar imports

Remover imports em todos os arquivos:

```python
# ‚ùå REMOVER
from app.core.cache import DocumentCacheManager
from app.core.cache import CacheKeyBuilder
```

#### 5.3 Atualizar `.gitignore`

```gitignore
# ‚ùå REMOVER linha
cache/
```

#### 5.4 Atualizar `README.md`

- ‚ùå Remover se√ß√£o "üíæ Azure Document Intelligence Cache System"
- ‚ùå Remover tabela de features do cache
- ‚ùå Remover exemplos de CLI
- ‚úÖ Adicionar se√ß√£o "üîç Verifica√ß√£o de Duplicatas MongoDB"

## üß™ Cen√°rios de Teste

### **Testes Unit√°rios (8h)**

#### 1. `test_duplicate_check_service.py`

```python
class TestDuplicateCheck:
    """Testes para verifica√ß√£o de duplicatas."""

    async def test_check_duplicate_returns_none_when_not_exists(self):
        """‚úÖ Retorna None quando documento n√£o existe."""
        pass

    async def test_check_duplicate_returns_record_when_exists_completed(self):
        """‚úÖ Retorna record quando documento existe com status COMPLETED."""
        pass

    async def test_check_duplicate_ignores_failed_documents(self):
        """‚úÖ Ignora documentos com status FAILED (permite retry)."""
        pass

    async def test_check_duplicate_uses_file_size(self):
        """‚úÖ Usa file_size na verifica√ß√£o (mesmo email+filename, size diferente = n√£o duplicado)."""
        pass

    async def test_check_duplicate_performance_with_index(self):
        """‚úÖ Verifica que query usa √≠ndice (EXPLAIN plan)."""
        pass
```

#### 2. `test_analyze_controller_duplicate.py`

```python
class TestAnalyzeWithDuplicate:
    """Testes de integra√ß√£o do endpoint com duplicatas."""

    async def test_upload_duplicate_returns_existing_data(self):
        """‚úÖ Retorna dados existentes sem reprocessar."""
        pass

    async def test_upload_same_name_different_size_reprocesses(self):
        """‚úÖ Reprocessa arquivo com mesmo nome mas tamanho diferente."""
        pass

    async def test_upload_failed_document_allows_retry(self):
        """‚úÖ Permite retry de documento que falhou anteriormente."""
        pass

    async def test_duplicate_response_format(self):
        """‚úÖ Response de duplicata tem formato correto."""
        pass
```

### **Testes de Integra√ß√£o (4h)**

#### 3. `test_duplicate_check_e2e.py`

```python
class TestDuplicateE2E:
    """Testes end-to-end do fluxo de duplicatas."""

    async def test_upload_process_duplicate_flow(self):
        """
        ‚úÖ Fluxo completo:
        1. Upload inicial ‚Üí processamento completo
        2. Upload duplicado ‚Üí retorna dados do passo 1
        3. Upload arquivo modificado (size diferente) ‚Üí reprocessa
        """
        pass

    async def test_mongodb_index_created(self):
        """‚úÖ Verifica que √≠ndice composto foi criado."""
        pass

    async def test_performance_duplicate_check(self):
        """‚úÖ Verifica√ß√£o de duplicata < 50ms (usando √≠ndice)."""
        pass
```

## üìà M√©tricas de Sucesso

| M√©trica                   | Antes (Cache)    | Depois (MongoDB) | Meta             |
| ------------------------- | ---------------- | ---------------- | ---------------- |
| **Tempo de verifica√ß√£o**  | ~50ms (arquivo)  | <50ms (√≠ndice)   | ‚úÖ <100ms        |
| **Uso de disco**          | ~15MB cache      | 0MB              | ‚úÖ Redu√ß√£o       |
| **Complexidade c√≥digo**   | 5 arquivos cache | 0 arquivos       | ‚úÖ Simplifica√ß√£o |
| **Duplicatas detectadas** | 80% (filename)   | 95% (+ size)     | ‚úÖ Melhoria      |
| **Testes passando**       | 229/229          | 241/241          | ‚úÖ +12 testes    |

## üö® Riscos e Mitiga√ß√µes

| Risco                             | Probabilidade | Impacto | Mitiga√ß√£o                                        |
| --------------------------------- | ------------- | ------- | ------------------------------------------------ |
| **√çndice MongoDB lento**          | Baixa         | Alto    | Testar performance antes, usar √≠ndice background |
| **Regress√£o em testes**           | M√©dia         | M√©dio   | Rodar suite completa antes de merge              |
| **File size = 0 em docs antigos** | Alta          | Baixo   | Migration define 0 para docs antigos (aceito)    |
| **Conflito com PR #25**           | Alta          | M√©dio   | Merge PR #25 primeiro, depois fazer esta issue   |

## üìù Checklist de Implementa√ß√£o

### Prepara√ß√£o

- [ ] Merge PR #25 (list-documents-endpoint)
- [ ] Criar branch `feature/remove-cache-add-duplicate-check`
- [ ] Executar testes baseline (229/229 passando)

### Desenvolvimento

- [ ] **Fase 1:** Migration + Model (2h)

  - [ ] Criar migration `2025_11_22_001000`
  - [ ] Adicionar campo `file_size` ao model
  - [ ] Rodar migration localmente
  - [ ] Testar √≠ndices criados

- [ ] **Fase 2:** Persistence Service (4h)

  - [ ] Adicionar m√©todo na interface
  - [ ] Implementar `check_duplicate_document()`
  - [ ] Testar query performance (< 50ms)
  - [ ] Adicionar logs estruturados

- [ ] **Fase 3:** DocumentExtractionService (6h)

  - [ ] Remover imports de cache
  - [ ] Simplificar m√©todo `get_extraction_data()`
  - [ ] Remover l√≥gica de cache
  - [ ] Atualizar testes existentes

- [ ] **Fase 4:** Controller (3h)

  - [ ] Adicionar verifica√ß√£o de duplicata
  - [ ] Implementar l√≥gica de retry (FAILED)
  - [ ] Formatar response de duplicata
  - [ ] Adicionar logs

- [ ] **Fase 5:** Limpeza (4h)
  - [ ] Deletar `app/core/cache/`
  - [ ] Deletar `cache_manager_cli.py`
  - [ ] Deletar `test_cache_system.py`
  - [ ] Deletar diret√≥rio `cache/`
  - [ ] Limpar imports em todos arquivos
  - [ ] Atualizar `.gitignore`

### Testes

- [ ] **Testes Unit√°rios** (8h)

  - [ ] `test_duplicate_check_service.py` (5 testes)
  - [ ] `test_analyze_controller_duplicate.py` (4 testes)
  - [ ] Atualizar testes existentes quebrados
  - [ ] Rodar suite completa: 241/241 ‚úÖ

- [ ] **Testes de Integra√ß√£o** (4h)
  - [ ] `test_duplicate_check_e2e.py` (3 testes)
  - [ ] Validar √≠ndices MongoDB
  - [ ] Testar performance (< 50ms)

### Documenta√ß√£o

- [ ] **README.md** (2h)

  - [ ] Remover se√ß√£o de Cache
  - [ ] Adicionar se√ß√£o de Duplicate Check
  - [ ] Atualizar fluxograma de processamento
  - [ ] Atualizar tabela de features

- [ ] **API.md**
  - [ ] Documentar response de duplicata
  - [ ] Adicionar exemplos

### Finaliza√ß√£o

- [ ] Rodar todos os testes: `pytest` ‚úÖ
- [ ] Rodar coverage: `pytest --cov` (>50%) ‚úÖ
- [ ] Executar migration em dev: ‚úÖ
- [ ] Validar performance: <50ms ‚úÖ
- [ ] Code review interno
- [ ] Criar PR com descri√ß√£o completa
- [ ] Aguardar review e merge

## üîó Depend√™ncias

- **Bloqueado por:** PR #25 (feature/list-documents-endpoint) - DEVE ser merged primeiro
- **Bloqueia:** Nenhuma issue conhecida

## üìö Refer√™ncias

- [MongoDB Index Documentation](https://www.mongodb.com/docs/manual/indexes/)
- [Pydantic Models](https://docs.pydantic.dev/)
- [FastAPI Best Practices](https://fastapi.tiangolo.com/tutorial/)
- Sistema atual de cache: `app/core/cache/cache_manager.py`
- Migration system: `scripts/migrations/`

## üë• Assignees

- **Desenvolvedor:** @Bergami
- **Reviewer:** A definir
- **QA:** A definir

## üè∑Ô∏è Labels

- `enhancement`
- `refactoring`
- `breaking-change`
- `high-priority`
- `mongodb`
- `performance`

## ‚è±Ô∏è Estimativa

**Total:** 34 horas (~4-5 dias √∫teis)

**Breakdown:**

- Desenvolvimento: 19h
- Testes: 12h
- Documenta√ß√£o: 2h
- Conting√™ncia: 1h

---

**Criado em:** 2025-11-21  
**Vers√£o:** 1.0  
**Status:** Ready for Development
