"""
ðŸŽ¯ DocumentAnalysisOrchestrator - FASE 3 SOLID Refactoring

Orquestrador especializado para coordenar todo o pipeline de anÃ¡lise de documentos.

Responsabilidades (SRP):
- Coordenar extraÃ§Ã£o de imagens com fallback
- Coordenar categorizaÃ§Ã£o de imagens  
- Coordenar parsing de header e questÃµes
- Coordenar construÃ§Ã£o de context blocks
- Agregar resultados finais

PrincÃ­pios SOLID Aplicados:
- SRP: Uma Ãºnica responsabilidade - orquestrar o pipeline
- OCP: ExtensÃ­vel via injeÃ§Ã£o de estratÃ©gias diferentes
- LSP: Pode ser substituÃ­do por orquestradores especializados
- ISP: Interfaces mÃ­nimas e especÃ­ficas
- DIP: Depende de abstraÃ§Ãµes, nÃ£o implementaÃ§Ãµes concretas
"""
import logging
from typing import Dict, Any, List
from uuid import uuid4
from fastapi import UploadFile

from app.parsers.header_parser import HeaderParser
from app.parsers.question_parser import QuestionParser
from app.services.image.interfaces.image_categorization_interface import ImageCategorizationInterface
from app.services.image.extraction.image_extraction_orchestrator import ImageExtractionOrchestrator
from app.services.azure.azure_figure_processor import AzureFigureProcessor
from app.services.context.refactored_context_builder import RefactoredContextBlockBuilder
from app.models.internal import (
    InternalDocumentResponse,
    InternalDocumentMetadata,
    InternalQuestion,
    InternalContextBlock,
    InternalImageData
)
from app.core.exceptions import DocumentProcessingError

logger = logging.getLogger(__name__)


class DocumentAnalysisOrchestrator:
    """
    ðŸŽ¯ Orquestrador especializado para anÃ¡lise completa de documentos.
    
    Esta classe coordena todo o pipeline de anÃ¡lise, aplicando o princÃ­pio SRP
    ao mÃ¡ximo - o AnalyzeService agora apenas valida e delega.
    
    Pipeline de AnÃ¡lise:
    1. ExtraÃ§Ã£o de imagens com fallback
    2. CategorizaÃ§Ã£o de imagens (header vs content)
    3. Parsing de header e metadados
    4. ExtraÃ§Ã£o de questÃµes dos parÃ¡grafos Azure
    5. ConstruÃ§Ã£o de context blocks refatorados
    6. AssociaÃ§Ã£o de figuras Ã s questÃµes
    7. AgregaÃ§Ã£o final da resposta
    """
    
    def __init__(self,
                 image_categorizer: ImageCategorizationInterface,
                 image_extractor: ImageExtractionOrchestrator,
                 context_builder: RefactoredContextBlockBuilder,
                 figure_processor: AzureFigureProcessor):
        """
        Dependency Injection aplicando DIP (Dependency Inversion Principle).
        
        Args:
            image_categorizer: Interface para categorizaÃ§Ã£o de imagens
            image_extractor: Orquestrador de extraÃ§Ã£o de imagens
            context_builder: Constructor de blocos de contexto refatorado
            figure_processor: Processador de figuras Azure
        """
        self._image_categorizer = image_categorizer
        self._image_extractor = image_extractor
        self._context_builder = context_builder
        self._figure_processor = figure_processor
        self._logger = logging.getLogger(__name__)

    async def orchestrate_analysis(self,
                                  extracted_data: Dict[str, Any],
                                  email: str,
                                  filename: str,
                                  file: UploadFile,
                                  use_refactored: bool = True) -> InternalDocumentResponse:
        """
        ðŸŽ¯ Orquestra todo o pipeline de anÃ¡lise de documento.
        
        Este Ã© o mÃ©todo principal que coordena todas as fases da anÃ¡lise,
        delegando para os serviÃ§os especializados.
        
        Args:
            extracted_data: Dados brutos extraÃ­dos pelo DocumentExtractionService
            email: Email do usuÃ¡rio
            filename: Nome do arquivo original
            file: Objeto UploadFile para fallback de extraÃ§Ã£o
            use_refactored: Flag para usar lÃ³gica de processamento avanÃ§ada
            
        Returns:
            InternalDocumentResponse: Resposta completa estruturada
            
        Raises:
            DocumentProcessingError: Em caso de falha no pipeline
        """
        document_id = str(uuid4())
        self._logger.info(f"ðŸŽ¯ Starting document analysis orchestration for {filename} (user: {email})")
        
        try:
            # Phase 1: PreparaÃ§Ã£o de dados bÃ¡sicos
            analysis_context = await self._prepare_analysis_context(
                extracted_data, email, filename, document_id
            )
            
            # Phase 2: ExtraÃ§Ã£o e categorizaÃ§Ã£o de imagens
            image_analysis = await self._execute_image_analysis_phase(
                file, analysis_context, document_id
            )
            
            # Phase 3: Parsing de header e metadados
            header_metadata = await self._execute_header_parsing_phase(
                analysis_context, image_analysis
            )
            
            # Phase 4: ExtraÃ§Ã£o de questÃµes
            questions_and_context = await self._execute_question_extraction_phase(
                analysis_context, image_analysis
            )
            
            # Phase 5: ConstruÃ§Ã£o de context blocks refatorados
            enhanced_context_blocks = await self._execute_context_building_phase(
                analysis_context, image_analysis, use_refactored
            )
            
            # Phase 6: AssociaÃ§Ã£o de figuras (se aplicÃ¡vel)
            enhanced_questions = await self._execute_figure_association_phase(
                analysis_context, questions_and_context["questions"]
            )
            
            # Phase 7: AgregaÃ§Ã£o final
            final_response = await self._aggregate_final_response(
                analysis_context,
                image_analysis,
                header_metadata,
                enhanced_questions,
                enhanced_context_blocks or questions_and_context["context_blocks"]
            )
            
            self._logger.info(f"âœ… Document analysis orchestration completed successfully for {filename}")
            return final_response
            
        except Exception as e:
            self._logger.error(f"âŒ Document analysis orchestration failed for {filename}: {str(e)}")
            raise DocumentProcessingError(f"Analysis pipeline failed: {str(e)}") from e

    async def _prepare_analysis_context(self,
                                      extracted_data: Dict[str, Any],
                                      email: str,
                                      filename: str,
                                      document_id: str) -> Dict[str, Any]:
        """
        Phase 1: Prepara o contexto bÃ¡sico para anÃ¡lise.
        """
        self._logger.info("ðŸ“‹ Phase 1: Preparing analysis context")
        
        extracted_text = extracted_data.get("text", "")
        azure_result = extracted_data.get("metadata", {}).get("raw_response", {})
        
        context = {
            "extracted_text": extracted_text,
            "azure_result": azure_result,
            "email": email,
            "filename": filename,
            "document_id": document_id,
            "provider_metadata": extracted_data.get("metadata", {})
        }
        
        self._logger.info(f"âœ… Phase 1 complete: Context prepared with Azure result: {bool(azure_result)}")
        return context

    async def _execute_image_analysis_phase(self,
                                          file: UploadFile,
                                          analysis_context: Dict[str, Any],
                                          document_id: str) -> Dict[str, Any]:
        """
        Phase 2: Executa extraÃ§Ã£o e categorizaÃ§Ã£o de imagens.
        """
        self._logger.info("ðŸ–¼ï¸ Phase 2: Executing image analysis (extraction + categorization)")
        
        # 2.1: ExtraÃ§Ã£o com fallback usando orquestrador especializado
        image_data = await self._image_extractor.extract_with_fallback(
            file=file,
            document_analysis_result=analysis_context["azure_result"],
            document_id=f"{analysis_context['email']}_{analysis_context['filename']}"
        )
        
        if image_data:
            self._logger.info(f"ðŸ–¼ï¸ Phase 2.1: {len(image_data)} images extracted successfully")
        else:
            self._logger.warning("âš ï¸ Phase 2.1: No images extracted")
            image_data = {}

        # 2.2: CategorizaÃ§Ã£o usando interface (DIP aplicado)
        header_images_pydantic, content_images_pydantic = [], []
        
        if isinstance(image_data, dict) and image_data:
            self._logger.info(f"ðŸ·ï¸ Phase 2.2: Categorizing {len(image_data)} images using Pydantic interface")
            
            header_images_pydantic, content_images_pydantic = self._image_categorizer.categorize_extracted_images(
                image_data, 
                analysis_context["azure_result"], 
                document_id=f"analyze_{len(image_data)}_images"
            )
            
            self._logger.info(f"âœ… Phase 2.2: Categorization complete - {len(header_images_pydantic)} header, {len(content_images_pydantic)} content")
        else:
            self._logger.info("â„¹ï¸ Phase 2.2: Skipping categorization - no valid image data")

        all_categorized_images = header_images_pydantic + content_images_pydantic
        
        result = {
            "image_data": image_data,
            "header_images": header_images_pydantic,
            "content_images": content_images_pydantic,
            "all_images": all_categorized_images
        }
        
        self._logger.info("âœ… Phase 2 complete: Image analysis finished")
        return result

    async def _execute_header_parsing_phase(self,
                                          analysis_context: Dict[str, Any],
                                          image_analysis: Dict[str, Any]) -> InternalDocumentMetadata:
        """
        Phase 3: Executa parsing do header e metadados.
        """
        self._logger.info("ðŸ“„ Phase 3: Executing header parsing")
        
        header_metadata = HeaderParser.parse_to_pydantic(
            header=analysis_context["extracted_text"],
            header_images=image_analysis["header_images"],
            content_images=image_analysis["content_images"]
        )
        
        self._logger.info("âœ… Phase 3 complete: Header metadata parsed")
        return header_metadata

    async def _execute_question_extraction_phase(self,
                                               analysis_context: Dict[str, Any],
                                               image_analysis: Dict[str, Any]) -> Dict[str, List]:
        """
        Phase 4: Executa extraÃ§Ã£o de questÃµes dos parÃ¡grafos Azure.
        """
        self._logger.info("â“ Phase 4: Executing question extraction")
        
        azure_result = analysis_context["azure_result"]
        image_data = image_analysis["image_data"]
        
        # Extrair parÃ¡grafos Azure
        azure_paragraphs = azure_result.get("paragraphs", []) if azure_result else []
        
        questions = []
        context_blocks = []
        
        if azure_paragraphs:
            self._logger.info(f"ðŸ“ Phase 4.1: Processing {len(azure_paragraphs)} Azure paragraphs")
            
            # Preparar parÃ¡grafos no formato esperado
            paragraph_list = [{"content": p.get("content", "")} for p in azure_paragraphs if p.get("content")]
            
            # Extrair usando mÃ©todo eficiente
            raw_data = QuestionParser.extract_from_paragraphs(paragraph_list, image_data)
            
            # Converter para Pydantic com validaÃ§Ã£o
            for i, q in enumerate(raw_data.get("questions", [])):
                try:
                    if not q.get("question"):
                        self._logger.warning(f"âš ï¸ Question {i+1} has empty content, skipping")
                        continue
                        
                    pydantic_q = InternalQuestion.from_legacy_question(q)
                    questions.append(pydantic_q)
                    self._logger.debug(f"âœ… Question {i+1} converted: {len(pydantic_q.content.statement)} chars")
                except Exception as e:
                    self._logger.error(f"âŒ Error converting question {i+1}: {e}")
                    continue
            
            for i, cb in enumerate(raw_data.get("context_blocks", [])):
                try:
                    pydantic_cb = InternalContextBlock.from_legacy_context_block(cb)
                    context_blocks.append(pydantic_cb)
                except Exception as e:
                    self._logger.warning(f"âš ï¸ Error converting context block {i+1}: {e}")
                    continue
            
            self._logger.info(f"âœ… Phase 4: Extracted {len(questions)} questions, {len(context_blocks)} context blocks")
        else:
            self._logger.error("âŒ Phase 4: No Azure paragraphs available - cannot extract questions")
            raise ValueError("Azure paragraphs are required for SOLID extraction")
        
        return {
            "questions": questions,
            "context_blocks": context_blocks
        }

    async def _execute_context_building_phase(self,
                                            analysis_context: Dict[str, Any],
                                            image_analysis: Dict[str, Any],
                                            use_refactored: bool) -> List[InternalContextBlock]:
        """
        Phase 5: Executa construÃ§Ã£o de context blocks refatorados.
        """
        if not use_refactored:
            self._logger.info("â„¹ï¸ Phase 5: Skipped - refactored context building disabled")
            return None
            
        self._logger.info("ðŸ”§ Phase 5: Executing refactored context building")
        
        azure_result = analysis_context["azure_result"]
        image_data = image_analysis["image_data"]
        
        if not azure_result:
            self._logger.warning("âš ï¸ Phase 5: No Azure result - skipping enhanced context building")
            return None
        
        try:
            self._logger.info("ðŸ”¥ Phase 5.1: Using parse_to_pydantic() - Native Pydantic Interface")
            
            enhanced_context_blocks = self._context_builder.parse_to_pydantic(azure_result, image_data)
            
            blocks_with_images = sum(1 for cb in enhanced_context_blocks if cb.has_images)
            self._logger.info(f"âœ… Phase 5: Created {len(enhanced_context_blocks)} context blocks ({blocks_with_images} with images)")
            
            return enhanced_context_blocks
            
        except Exception as e:
            self._logger.warning(f"ðŸ”„ Phase 5: parse_to_pydantic failed ({e}), using legacy method")
            
            enhanced_context_blocks_dict = self._context_builder.build_context_blocks_from_azure_figures(
                azure_result, image_data
            )
            
            if enhanced_context_blocks_dict:
                context_blocks = [
                    InternalContextBlock.from_legacy_context_block(cb) 
                    for cb in enhanced_context_blocks_dict
                ]
                self._logger.info(f"âœ… Phase 5: Created {len(context_blocks)} context blocks (legacy method)")
                return context_blocks
            else:
                self._logger.warning("âš ï¸ Phase 5: No enhanced context blocks created")
                return None

    async def _execute_figure_association_phase(self,
                                              analysis_context: Dict[str, Any],
                                              questions: List[InternalQuestion]) -> List[InternalQuestion]:
        """
        Phase 6: Executa associaÃ§Ã£o de figuras Ã s questÃµes.
        """
        self._logger.info("ðŸ–¼ï¸ Phase 6: Executing figure association")
        
        azure_result = analysis_context["azure_result"]
        
        try:
            processed_figures = AzureFigureProcessor.process_figures_from_azure_response(azure_result)
            
            enhanced_questions = AzureFigureProcessor.associate_figures_to_pydantic_questions(
                processed_figures, questions
            )
            
            self._logger.info("âœ… Phase 6: Questions enhanced with figure associations")
            return enhanced_questions
            
        except Exception as e:
            self._logger.warning(f"ðŸ”„ Phase 6: Figure association failed: {e}, proceeding without enhancement")
            return questions

    async def _aggregate_final_response(self,
                                      analysis_context: Dict[str, Any],
                                      image_analysis: Dict[str, Any],
                                      header_metadata: InternalDocumentMetadata,
                                      questions: List[InternalQuestion],
                                      context_blocks: List[InternalContextBlock]) -> InternalDocumentResponse:
        """
        Phase 7: Agrega todos os resultados na resposta final.
        """
        self._logger.info("ðŸ“¦ Phase 7: Aggregating final response")
        
        response = InternalDocumentResponse(
            email=analysis_context["email"],
            document_id=analysis_context["document_id"],
            filename=analysis_context["filename"],
            document_metadata=header_metadata,
            questions=questions,
            context_blocks=context_blocks,
            extracted_text=analysis_context["extracted_text"],
            provider_metadata=analysis_context["provider_metadata"],
            all_images=image_analysis["all_images"]
        )
        
        self._logger.info(f"âœ… Phase 7 complete: Final response aggregated with {len(questions)} questions, {len(context_blocks)} context blocks")
        return response